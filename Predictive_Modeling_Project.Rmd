---
title: "Predictive Modeling Project"
author: "Kaili Saffran"
date: "11/11/2018"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
 
#Section I. Introduction
I am interested in researching passing rate for NFL quarterbacks (QB). The outcome variable I am interested in predicting is QB rating. I have taken data from a free public data site, OpenDataSoft, to retrieve statistics from every game of the 2016 season that is stored in a csv file. I will use these statistics to predict QB rating for the 2017 NFL season and see how important some of the predictors are in determining the QB rating. Being able to predict QB rating accurately is important because the salary of each quarterback is determined based on this rating and it helps determine ranking for fantasy football picks and NFL drafts.

The QB Rating variable is continuous; therefore, I will be using regression methods to perform my model. QB rating is measured as a numeric value that scales from 0 to 158.3 based on completion percentage, yards per attempt, touchdowns per attempt, and interceptions per attempt based on a calculator that is used to determine QB rating. I will be using predictors that do not include any of the variables from this calculator because using any of those variables will result in an r-squared value of 1 and will not result in a useful predictive model. It is already determined that these five predictors do directly effect QB rating. 

The predictors I will potentially look at include, 1st down passing yards, 1st down percentage, longest pass, sacks, sack yards, fumbles, and fumbles lost. The measurement for these predictors are all numeric ranges that have some range between 0 and 100. All the predictors I want to work with are continuous as well.

#Section II. Pre-Processing.
```{r}
library(dplyr)
library(caret)
library(car)
library(pastecs)
nfl=read.csv("/Users/kailisaffran/Documents/Exploratory Data Analysis/NFL Passing Data.csv")
nfl.new=nfl[,c(10,11,12,13,14,15,16,17)]
newdat=na.omit(nfl.new)
set.seed(1025)
```

```{r}
#Identify Outliers
model=lm(QB.Rating~.,data=newdat)
summary(model)
influenceIndexPlot(model,vars=c("Cook"),id.method=list(n=5))
summary(update(model,subset=-c(5)))
```

The data I retrieved was organized in a table, so to begin, I had to copy and paste the data into a csv file in excel. I then edited my csv file to only include numeric variables and the outcome variable and predictors that were not known to be highly correlated to QB rating. The highly correlated predictors are those used in the QB rating calculator.. Finally, I omitted any missing data in my new csv file to get the final data file I will be using in the data analysis for my model and set a seed to make sure when I run my code I am always working with the same model. By looking at influential points from the variables I want to work with, Cook's plot shows only a few potential outliers. Therefore, I will not adjust my data set to not include these outliers. All outliers are important to this data set because they help determine the QB rating. Taking them away can change the QB rating and every game counts when ranking in the NFL. A QB that has some off games is important to track when determining rankings because it shows how consistent of a player they are. In the summary, it shows that re-running the model without these influential cases does not change the residual standard error significantly. Therefore, eliminating outliers is not necessary.

#Section III. Exploratory Data Analysis
##Outcome Variable Distribution
```{r}
round(stat.desc(newdat$QB.Rating,basic=FALSE,norm=TRUE), digits=3)
boxplot(newdat$QB.Rating, notch=T, horizontal=T, xlab="Rating from 0-158.3", main="QB.Rating") 
hist(newdat$QB.Rating, xlim=range(0,158,by=10), xlab="Rating from 0-158.3")
```
The outcome variable, QB.Rating, is almost normally distributed. This is also supported by the mean and median values which are pretty much equal. There are a few outliers that lie below a QB rating of 10 shown in the box plot. The boxplot also shows the minimum value to be around 15 and the maximum to be about the maximum of the range, 158.3. Additionally, quartile 1 is about 70 and quartile 3 is about 105.

```{r}
plot(density(newdat$QB.Rating)) 
rug(newdat$QB.Rating)
qqnorm(newdat$QB.Rating)
qqline(newdat$QB.Rating)
```
The density plot shows a fairly smooth distribution of QB.Rating as well. The rug on the bottom of the plot shows where there is higher frequency. The darker the shading, the more data points there are at those values. It shows that the majority of the data lies at QB ratings between 75 and 125 roughly. The Q-Q plot shows the normality of the outcome variable. Since, for the most part, the data forms a solid dark line, other than on the very endpoints of the range from 0-158.3, the outcome variable can be said to almost be normally distributed and skewed towards the higher end of the QB rating range.

```{r}
library(Hmisc)
nfl2=cut2(newdat$QB.Rating,g=2) 
table(nfl2)
barplot(table(nfl2))
```
QB.Rating is also supported to be pretty evenly distributed with a barplot. The barplot divides the QB rating data into two even groups that is split at the median. There are 308 values in the first group and 302 values in the second group which is almost equal. They are split at the median which is at 87 which means the data is slightly skewed towards the higher end of the QB rating range. However, it is not necessary to split the variable into groups since it is almost evenly distributed. Splitting a continuous variable into quantile-based groups is for variables with lots of outliers and skewness.

##Predictors with Outcome Variable Distribution
```{r}
pairs(newdat)
scatterplotMatrix(newdat, plot.points=TRUE, smoother=loessLine) 

plot(QB.Rating~X1st.Down.Passing+Longest.Pass+Sacks+Fumbles, data=newdat)
```
The scatterplotmatrix shows very strong correlation in almost all of these potential predictors because there are not many outliers. Sack yards, fumbles, and fumbles lost are the least correlated variables because they have more outliers and are not as evenly distributed. For QB.Rating vs. X1st.Down.Passing, the highest frequency is between passing yards of 10 and 15 and a QB rating between 50 and 100. The plot is represented with columns because passing yards are rounded to the nearest whole number. For QB.Rating vs. Longest.Pass, the  highest frequency lies between a longest pass of 20 and 60 yards and a QB rating between 50 and 100. This plot is more scattered because the yards for longest pass have a greater range. For QB.Rating vs. Sacks, the highest frequency lies between 0 and 4 sacks and a QB rating between 50 and 120. The range of sacks is not very large, so this plot is arranged uniformly in columns. The same reasoning applies to QB.Rating vs. Fumbles which has a high frequency between 0 and 1 fumbles and a QB rating between 40 and 140.

###Predictor 1
```{r}
plot(newdat$X1st.Down.Passing,newdat$QB.Rating)
scatter.smooth(newdat$X1st.Down.Passing,newdat$QB.Rating)
scatterplot(QB.Rating~X1st.Down.Passing,data=newdat)

library(ISLR)
stat1=table(newdat$X1st.Down.Passing,newdat$QB.Rating,dnn=list("X1st.Down.Passing","QB.Rating"))
library(vcd) 
assocstats(stat1) 
```
The scattersmooth plot shows that relatively, as 1st Down Passing Yards increase, so does QB.Rating. The scatterplot shows confidence intervals and where the majority of the data lies. A contingency coefficient of 0.973 indicates very high correlation between QB.Rating and X1st.Down.Passing.

###Predictor 2
```{r}
plot(newdat$Longest.Pass,newdat$QB.Rating)
scatter.smooth(newdat$Longest.Pass,newdat$QB.Rating)
scatterplot(QB.Rating~Longest.Pass,data=newdat)

stat2=table(newdat$Longest.Pass,newdat$QB.Rating,dnn=list("Longest.Pass","QB.Rating")) 
assocstats(stat2) 
```
Relatively, as the Longest.Pass value increases, so does QB.Rating. The majority of the data lies between a longest pass of 20 to 60 yards and a QB rating between 50 and 100. A contingency coefficient of 0.992 indicates very high correlation between QB.Rating and Longest.Pass.

###Predictor 3
```{r}
plot(newdat$Sacks,newdat$QB.Rating)
scatter.smooth(newdat$Sacks,newdat$QB.Rating)
scatterplot(QB.Rating~Sacks,data=newdat)

stat3=table(newdat$Sacks,newdat$QB.Rating,dnn=list("Sacks","QB.Rating"))
assocstats(stat3) 
```
After 2 sacks, as the number of sacks increase, QB.Rating decreases. A small number of sacks does not necessarily affect QB.Rating because a sack can be the result of playing against a strong defense or having a weak offensive line. There are a greater number of outliers with the Sacks variable because there is less variance. A contingency coefficient of 0.928 indicates high correlation between QB.Rating and Sacks.

###Predictor 4
```{r}
plot(newdat$Fumbles,newdat$QB.Rating)
scatter.smooth(newdat$Fumbles,newdat$QB.Rating)
scatterplot(QB.Rating~Fumbles,data=newdat)

stat4=table(newdat$Fumbles,newdat$QB.Rating,dnn=list("Fumbles","QB.Rating")) 
assocstats(stat4) 
```
After 1 fumble, the QB.Rating decreases. Just as with sacks, a small number of fumbles cannot affect QB.Rating too much if playing a team with a strong defense. There is low variance in this predictor as well. A contingency coefficient of 0.867 indicates high correlation between QB.Rating and Fumbles.

From conducting exploratory data analysis, we can see that all these predictors are highly correlated to QB.Rating. They should be modeled and analyzed so it can be determined whether these variables should also be included in how the QB rating is calculated in the NFL. Because the predictors I wish to work with are so highly correlated, it may result in an uninteresting model. I will use kNN regression and regression trees to build my models because my variables are continuous, I am working with a large data set, and this will further break the predictors down to really see how correlated they are to my outcome variable and which predictors are most important. 

#Section IV. Model Fitting and Analysis
```{r}
set.seed(1025)
new=createDataPartition(y=newdat$QB.Rating,p=.5,list=FALSE) 
train=newdat[new,]
test=newdat[-new,]
dim(train); dim(test)
```
I am partitioning the data so that half of the data goes into the training data and half goes into the test data so one side is not more weighted than the other. I felt this would be reliable because of how big of a data set I am working with. I believe this will create the best predictive model.

##kNN Regression One Variable
```{r}
tc1=trainControl(method="boot",number=20)

knnfit1=train(QB.Rating~Longest.Pass, data=train, method="knn",
                trControl=tc1,
                preProcess=c("center","scale"),
                tuneLength=10)
knnfit1 
plot(knnfit1)
```
I set the boostrapping number of resampling iterations to 20 because I wanted a high value of k to produce a smoother and more variable fit, but too high of a k value can produce bias. By using tunelength to run the kNN model, the model can try running 10 different k values to find the optimal k value. I found that the optimal k value for this model is 23, which may indicate that there is too much bias. Therefore, I will now specify k using smaller boundaries after taking the square root of the summary of sample sizes which is 307. The new k value I got was 17.5, so I will adjust k according to this new value. R-squared is somewhere around 0.2 for all k values which means there is about 20% of variance in QB.Rating and about 20% of the data is explained by Longest.Pass.

```{r}
grid=data.frame(k=c(15,17,19))
knnfit2=train(QB.Rating~Longest.Pass, data=train, method="knn",
             trControl=tc1,
             preProcess=c("center","scale"),
             tuneGrid=grid) 
knnfit2
plot(knnfit2)
(RMSEtest1<-sqrt(mean((predict(knnfit2,newdata=test)-test$QB.Rating)^2))) 
```
RMSE for this model is 27.333.

```{r}
lmfit1=train(QB.Rating~Longest.Pass,data = train,method="lm",trControl=tc1)
(RMSEtest2<-sqrt(mean((predict(lmfit1,newdata=test)-test$QB.Rating)^2))) 

lmfit2=train(QB.Rating~poly(Longest.Pass,2),data = train,method="lm",trControl=tc1) #Quadratic Model
(RMSEtest3<-sqrt(mean((predict(lmfit2,newdata=test)-test$QB.Rating)^2))) 

newx1<-data.frame(Longest.Pass=20)
predict(lmfit2,newx1)
newx2<-data.frame(Longest.Pass=50)
predict(lmfit2,newx2)
newx3<-data.frame(Longest.Pass=65)
predict(lmfit2,newx3)
```
The RMSE for using a linear model is 26.991 which shows to be better than the kNN model. The RMSE for using a quadratic model is slightly better at 26.823, therefore the quadratic model is the best test for prediciting the model. For predicting QB.Rating based on Longest.Pass, I will use the quadratic model since it showed to have the lowest RMSE. For a longest pass of 20 yards, the QB rating is predicted to be 74.5. For a longest pass of 50 yards, the QB rating is predicted to be 101.2. For a longest pass of 65 yards, the QB rating is predicted to be 105.7.

##kNN Regression Multiple Variables
```{r}
tc2=trainControl(method="boot",number=20)

knnfit3=train(QB.Rating~X1st.Down.Passing+Longest.Pass+Sacks+Fumbles, data=train, method="knn",
                trControl=tc2,
                preProcess=c("center","scale"),
                tuneLength=10)
knnfit3
plot(knnfit3)
```
The optimal value of k for the model with multiple predictors is 17. Now k will be modified to get a more accurate RMSE based on specifying to the value of 17 I have received. R-squared is somewhere around 0.25 for almost all k values which means there is about 25% of variance in QB.Rating and about 25% of the data is explained by the four predictors I used.

```{r}
grid=data.frame(k=c(17,19,21))
knnfit4=train(QB.Rating~X1st.Down.Passing+Longest.Pass+Sacks+Fumbles,data=train, method="knn",
             trControl=tc2,
             preProcess=c("center","scale"),
             tuneGrid=grid) 
knnfit4
plot(knnfit4)
(RMSEtest4<-sqrt(mean((predict(knnfit4,newdata=test)-test$QB.Rating)^2))) 
```
The RMSE for the kNN model is 27.089, which is slightly better than the RMSE of 27.333 that was found in the kNN model with only one predictor. 

```{r}
lmfit3=train(QB.Rating~X1st.Down.Passing+Longest.Pass+Sacks+Fumbles,data = train,method="lm",trControl=tc2) 
(RMSEtest5<-sqrt(mean((predict(lmfit3,newdata=test)-test$QB.Rating)^2))) 

lmfit4=train(QB.Rating~poly(X1st.Down.Passing+Longest.Pass+Sacks+Fumbles,2),data = train,method="lm",trControl=tc2)
(RMSEtest6<-sqrt(mean((predict(lmfit4,newdata=test)-test$QB.Rating)^2))) 
```
The RMSE for the linear model is 25.993 which is better than the kNN model for multiple predictors and better than the linear model with only one predictor, Longest.Pass. The quadratic model RMSE is 26.763 which is worse than the linear model, but still better than the kNN model, which shows that the linear model is the best representation for this model working with several predictors. Overall, the models with multiple predictors have lower RMSEs than the model with only one predictor, which means QB.Rating is predicted more accurately using all of these predictors together rather than with just one, in this case, Longest.Pass.

```{r}
predlm1=predict(lmfit3, newdata=test)
(qbrat1=cbind(test$X1st.Down.Passing,test$QB.Rating,predlm1))

predlm2=predict(lmfit3, newdata=test)
(qbrat2=cbind(test$Sacks,test$QB.Rating,predlm2))

predlm3=predict(lmfit3, newdata=test)
(qbrat3=cbind(test$Fumbles,test$QB.Rating,predlm3))
```
To test my remaining variables, I used the linear model from my model with several variables since it had the lowest RMSE and produced predicted values from rows of my test data.

##Regression Tree
```{r}
library(randomForest)
tc3 <- trainControl(method="cv", number = 6) 
fit1=train(QB.Rating~.,method="rpart",trControl=tc3, data=train) 
fit1
```
I used the same data partioning split as the model for kNN regression. For regression trees, we work with the whole data set because applying a regression tree to a model with 5 or less predictors results in overfitting the data. I am working with a 6-fold cross validation. Performing a regression tree generates cp values which is a complexity parameter. It calculated an optimal cp value of 0.064 which has the smallest RMSE of 21.696. This cp's r-squared value is the highest as well which implies that this cp explains most of the model compared to the other cp values. We can see that the resampling of 20 bootstraps was applied as well.

```{r}
library(rattle)
fancyRpartPlot(fit1$finalModel)
```
This regression tree shows which predictors are most important in determining QB.Rating. In the regression tree, it can be seen that 1st down passing yards is the most important predictor. This could have been predicted after running exploratory analysis on my predictors and outcome variable because X1st.Down.Passing and QB.Rating had the highest correlation. Starting at the top of the tree, 87 is the overall average QB rating out of all 307 samples. When 1st down passing yards are less than 28, the average QB rating is 55 from a number of 75 cases. When 1st down passing yards are greater than or equal to 28, the average QB rating is 97 from a total of 232 cases. From these 232 cases, QB.Rating is further predicted by 1st down passing yards. Now, if 1st down passing yards are less than 43, the average QB rating is 90 from a total of 173 cases. Finally, if 1st down passing yards are greater than or equal to 43, the average QB rating is 117 by a number of 59 cases.

```{r}
(RMSEtest7<-sqrt(mean((predict(fit1,newdata=test)-test$QB.Rating)^2)))
(test$predsalrf=predict(fit1,test))
```
The RMSE for the regression tree model is 23.092. This RMSE is significantly lower than the RMSEs from the kNN regression models. Therefore, a regression tree is a better fit for predicting QB.Rating. In predicting QB.Rating from this model, each case is represented with either a QB.Rating of 55, 90, or 117. The average of these produce the total average QB ratings of 87 and 97.

```{r}
fitrf=train(QB.Rating~.,method="rf",data=train,trControl=tc3)
(RMSEtest8<-sqrt(mean((predict(fitrf,newdata=test)-test$QB.Rating)^2)))
(test$predsalrf=predict(fitrf,test))
```
By running a random forest tree to calculate RMSE, I retrieved an even better RMSE for my model at 21.904. A random forest produces a better RMSE than a normal regression tree because it is more powerful. Each mtry value has a lower RMSE value and higher r-squared value than all of my other previous models as well showing this model's strength. Predicting QB.Rating from a random forest tree model produces much better predicted results and less similar answers.

#Section V. Summary
Through performing a kNN regression and regression tree model, I found that the predictors I selected are important for determining QB.Rating and are highly correlated to one another. Both models produced fairly low RMSEs, showing that they performed relatively well in predicting my outcome variable. The predictors I chose influence QB.Rating the way I intended them to, however, they cannot all be a true representaion of QB rating because some are not clear indicators of how much potential and skill a QB has. A QB's longest pass shows their throwing distance ability, but other QB's may not have that opportunity if their receivers don't get open. Additionally, sacks and fumbles are normally the result of playing against a strong defensive team. Finally, 1st down passing yards could potentially be included in the calculation, but a more important predictor could be 3rd down passing yards/completions. But, this predictor was not included in the data set I found.

Limitations to this model are influenced by how the data was collected. We are not sure how accurate the data is from the site I retreived it from and who collected it. The data is recent enough to be significant, although statistics from the 2017 NFL season would have been even better for predictions. Some predictors that could be useful and that limit my model are QB running yards, touchdown passes, and 3rd down completions. These predictors would make the model a lot more reliable and efficient because they are significant in determining how a QB reacts under pressure. A QB runs when he has no other options, touchdown passes are important in scoring, and 3rd down completions are necessary to get to a 1st down.
